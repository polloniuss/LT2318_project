Berenice

AICS project:

- examine existing datasets (get access to them) for multi-modal deception detection
- check the existing annotations between images and texts - which visual features are useful for better deception detection?

- the idea is not necessarily about classifying / identifying object detection, but could be about training an agent to behave as if this agent is deceiving
- for example, based on (i) visual features (i) and previous text, generate the next sentence that is supposed to deceive
- the interesting question could be about the importance/help from all differently fine-grained features (e.g., eye-brows, mouth open, etc) in generating particular replies/texts

- an interesting investigation would be the effect of including/excluding multi-modal features in generating the sentences in these dialogues
- since the dialogues are all about trying to lie and be deceiving, one can later evaluate this model with humans in the lab (or student peers) and see whether the model that generates text from text only or also from other multi-modal features is perceived as more deceiving or not